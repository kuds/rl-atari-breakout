{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM3Z2pm9TMt0GM3hTsGMuPl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kuds/rl-atari-breakout/blob/main/%5BAtari%20Breakout%5D%20Model-Based%20Reinforcement%20Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using Model-Based Reinforcement Learning to play Atari's Breakout\n",
        "\n",
        "## References/Repositories\n",
        "- [Model-Based Reinforcement Learning for Atari - Simplified Repo](https://github.com/dhruvramani/model-based-atari/tree/master)\n",
        "- [Model-Based Reinforcement Learning for Atari - Paper](https://arxiv.org/abs/1903.00374)\n",
        "- [Tensor2Tensor - RL Code](https://github.com/tensorflow/tensor2tensor/tree/master/tensor2tensor/rl)"
      ],
      "metadata": {
        "id": "x5SgWAjNeWV5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install swig"
      ],
      "metadata": {
        "id": "qQxl2KnBqREe",
        "outputId": "b68b5479-a3d6-4c3b-fef7-60d6aa5c88c1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting swig\n",
            "  Downloading swig-4.2.1.post0-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl.metadata (3.5 kB)\n",
            "Downloading swig-4.2.1.post0-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.8 MB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.8 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.6/1.8 MB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: swig\n",
            "Successfully installed swig-4.2.1.post0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gymnasium[box2d]"
      ],
      "metadata": {
        "id": "U9ExFfgAuHui",
        "outputId": "bf28cfbc-40ee-4237-875a-ef681dcb181b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gymnasium[box2d] in /usr/local/lib/python3.10/dist-packages (1.0.0)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (0.0.4)\n",
            "Collecting box2d-py==2.3.5 (from gymnasium[box2d])\n",
            "  Downloading box2d-py-2.3.5.tar.gz (374 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.4/374.4 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pygame>=2.1.3 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (2.6.1)\n",
            "Requirement already satisfied: swig==4.* in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (4.2.1.post0)\n",
            "Building wheels for collected packages: box2d-py\n",
            "  Building wheel for box2d-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for box2d-py: filename=box2d_py-2.3.5-cp310-cp310-linux_x86_64.whl size=2376096 sha256=889c52bcd641604bf05383e5f35f3f0d4b3fa5cb06ecaef13f73431c287d3b2c\n",
            "  Stored in directory: /root/.cache/pip/wheels/db/8f/6a/eaaadf056fba10a98d986f6dce954e6201ba3126926fc5ad9e\n",
            "Successfully built box2d-py\n",
            "Installing collected packages: box2d-py\n",
            "Successfully installed box2d-py-2.3.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "30ciUtqduAiJ"
      },
      "outputs": [],
      "source": [
        "import gymnasium\n",
        "import platform\n",
        "import tensorflow\n",
        "import torch\n",
        "import numpy\n",
        "from importlib.metadata import version\n",
        "from datetime import datetime\n",
        "import google.colab.drive\n",
        "import itertools\n",
        "import multiprocessing\n",
        "from tensorflow.summary import SummaryWriter"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Python Version: {platform.python_version()}\")\n",
        "print(f\"Torch Version: {version('torch')}\")\n",
        "print(f\"Is Cuda Available: {torch.cuda.is_available()}\")\n",
        "print(f\"Cuda Version: {torch.version.cuda}\")\n",
        "print(f\"Gymnasium Version: {version('gymnasium')}\")\n",
        "print(f\"Numpy Version: {version('numpy')}\")\n",
        "print(f\"Tensorflow Version: {version('tensorflow')}\")\n",
        "print(f\"Swig Version: {version('swig')}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M-RRgny-u1zx",
        "outputId": "8c26e84c-f19b-4682-bee0-e684da37842d"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python Version: 3.10.12\n",
            "Torch Version: 2.5.0+cu121\n",
            "Is Cuda Available: False\n",
            "Cuda Version: 12.1\n",
            "Gymnasium Version: 1.0.0\n",
            "Numpy Version: 1.26.4\n",
            "Tensorflow Version: 2.17.0\n",
            "Swig Version: 4.2.1.post0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env_str = \"CarRacing-v3\"\n",
        "log_dir = \"./logs/{}\".format(env_str)\n",
        "env_kwargs_dict={\"continuous\": False}\n",
        "gray_scale = True"
      ],
      "metadata": {
        "id": "cqF9b9iKpwWQ"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Config:\n",
        "    def __init__(self, **overrides):\n",
        "        # Default values for all arguments\n",
        "        self.train_world_model = True\n",
        "        self.eval_world_model = True\n",
        "        self.num_rewards = 1\n",
        "        self.n_envs = 16\n",
        "        self.is_policy = False\n",
        "        self.has_rewards = False\n",
        "        self.hidden_size = 64\n",
        "        self.n_layers = 6\n",
        "        self.dropout_p = 0.1\n",
        "        self.max_ep_len = 500000\n",
        "        self.l2_clip = 0.0\n",
        "        self.softmax_clip = 0.03\n",
        "        self.reward_coeff = 0.1\n",
        "        self.log_interval = 100\n",
        "        self.activation_fn = 'relu'\n",
        "        self.log_dir = './log'\n",
        "        self.model_dir = './models'\n",
        "        self.world_model_path = \"CarRacingWorldModel\"\n",
        "        self.total_timesteps = int(1e6)\n",
        "        self.max_eval_iters = int(1e3)\n",
        "        self.render = True\n",
        "        self.debug = False\n",
        "\n",
        "        # Override defaults with any provided values\n",
        "        self._apply_overrides(overrides)\n",
        "\n",
        "    def _apply_overrides(self, overrides):\n",
        "        \"\"\"Apply any overrides passed in as keyword arguments.\"\"\"\n",
        "        for key, value in overrides.items():\n",
        "            if hasattr(self, key):\n",
        "                setattr(self, key, value)\n",
        "            else:\n",
        "                raise AttributeError(f\"Unknown configuration attribute: {key}\")\n",
        "\n",
        "    @classmethod\n",
        "    def from_argparse(cls):\n",
        "        \"\"\"Create a Config instance from command line arguments.\"\"\"\n",
        "        parser = argparse.ArgumentParser(\n",
        "            \"Model-Based Reinforcement Learning for Atari\",\n",
        "            formatter_class=argparse.ArgumentDefaultsHelpFormatter\n",
        "        )\n",
        "\n",
        "        # Define the same arguments as before\n",
        "        parser.add_argument('--train_world_model', type=str2bool, default=True)\n",
        "        parser.add_argument('--eval_world_model', type=str2bool, default=True)\n",
        "        parser.add_argument('--num_rewards', type=int, default=1)\n",
        "        parser.add_argument('--n_envs', type=int, default=16)\n",
        "        parser.add_argument('--is_policy', type=str2bool, default=False)\n",
        "        parser.add_argument('--has_rewards', type=str2bool, default=False)\n",
        "        parser.add_argument('--hidden_size', type=int, default=64)\n",
        "        parser.add_argument('--n_layers', type=int, default=6)\n",
        "        parser.add_argument('--dropout_p', type=float, default=0.1)\n",
        "        parser.add_argument('--max_ep_len', type=int, default=500000)\n",
        "        parser.add_argument('--l2_clip', type=float, default=0.0)\n",
        "        parser.add_argument('--softmax_clip', type=float, default=0.03)\n",
        "        parser.add_argument('--reward_coeff', type=float, default=0.1)\n",
        "        parser.add_argument('--log_interval', type=int, default=100)\n",
        "        parser.add_argument('--activation_fn', type=str, default='relu', choices=['relu', 'tanh'])\n",
        "        parser.add_argument('--log_dir', type=str, default='./log')\n",
        "        parser.add_argument('--model_dir', type=str, default='./models')\n",
        "        parser.add_argument('--world_model_path', type=str, default=\"CarRacingWorldModel\")\n",
        "        parser.add_argument('--total_timesteps', type=int, default=int(1e6))\n",
        "        parser.add_argument('--max_eval_iters', type=int, default=int(1e3))\n",
        "        parser.add_argument('--render', type=str2bool, default=True, help='Render frames')\n",
        "        parser.add_argument('--debug', type=str2bool, default=False, help='See debugging info')\n",
        "\n",
        "        args = parser.parse_args()\n",
        "        return cls(**vars(args))\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f\"{self.__class__.__name__}({self.__dict__})\"\n"
      ],
      "metadata": {
        "id": "yxIqwe1gmrTo"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def printstar(string, num_stars=50):\n",
        "    print(\"*\" * num_stars)\n",
        "    print(string)\n",
        "    print(\"*\" * num_stars)\n",
        "\n",
        "def make_env():\n",
        "    def _thunk():\n",
        "        env = gymnasium.make(env_str)\n",
        "        return env\n",
        "    return _thunk\n",
        "\n",
        "def worker(remote, parent_remote, env_fn_wrapper):\n",
        "    parent_remote.close()\n",
        "    env = env_fn_wrapper.x()\n",
        "    while True:\n",
        "        cmd, data = remote.recv()\n",
        "        if cmd == 'step':\n",
        "            ob, reward, done, info = env.step(data)\n",
        "            if done:\n",
        "                ob = env.reset()\n",
        "            remote.send((ob, reward, done, info))\n",
        "        elif cmd == 'reset':\n",
        "            ob = env.reset()\n",
        "            remote.send(ob)\n",
        "        elif cmd == 'reset_task':\n",
        "            ob = env.reset_task()\n",
        "            remote.send(ob)\n",
        "        elif cmd == 'close':\n",
        "            remote.close()\n",
        "            break\n",
        "        elif cmd == 'get_spaces':\n",
        "            remote.send((env.observation_space, env.action_space))\n",
        "        else:\n",
        "            raise NotImplementedError\n",
        "\n",
        "class VecEnv(object):\n",
        "    \"\"\"\n",
        "    An abstract asynchronous, vectorized environment.\n",
        "    \"\"\"\n",
        "    def __init__(self, num_envs, observation_space, action_space):\n",
        "        self.num_envs = num_envs\n",
        "        self.observation_space = observation_space\n",
        "        self.action_space = action_space\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"\n",
        "        Reset all the environments and return an array of\n",
        "        observations, or a tuple of observation arrays.\n",
        "        If step_async is still doing work, that work will\n",
        "        be cancelled and step_wait() should not be called\n",
        "        until step_async() is invoked again.\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    def step_async(self, actions):\n",
        "        \"\"\"\n",
        "        Tell all the environments to start taking a step\n",
        "        with the given actions.\n",
        "        Call step_wait() to get the results of the step.\n",
        "        You should not call this if a step_async run is\n",
        "        already pending.\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    def step_wait(self):\n",
        "        \"\"\"\n",
        "        Wait for the step taken with step_async().\n",
        "        Returns (obs, rews, dones, infos):\n",
        "         - obs: an array of observations, or a tuple of\n",
        "                arrays of observations.\n",
        "         - rews: an array of rewards\n",
        "         - dones: an array of \"episode done\" booleans\n",
        "         - infos: a sequence of info objects\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    def close(self):\n",
        "        \"\"\"\n",
        "        Clean up the environments' resources.\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    def step(self, actions):\n",
        "        self.step_async(actions)\n",
        "        return self.step_wait()\n",
        "\n",
        "\n",
        "class CloudpickleWrapper(object):\n",
        "    \"\"\"\n",
        "    Uses cloudpickle to serialize contents (otherwise multiprocessing tries to use pickle)\n",
        "    \"\"\"\n",
        "    def __init__(self, x):\n",
        "        self.x = x\n",
        "    def __getstate__(self):\n",
        "        import cloudpickle\n",
        "        return cloudpickle.dumps(self.x)\n",
        "    def __setstate__(self, ob):\n",
        "        import pickle\n",
        "        self.x = pickle.loads(ob)\n",
        "\n",
        "class SubprocVecEnv(VecEnv):\n",
        "    def __init__(self, env_fns, spaces=None):\n",
        "        \"\"\"\n",
        "        envs: list of gym environments to run in subprocesses\n",
        "        \"\"\"\n",
        "        self.waiting = False\n",
        "        self.closed = False\n",
        "        nenvs = len(env_fns)\n",
        "        self.nenvs = nenvs\n",
        "        self.remotes, self.work_remotes = zip(*[mp.Pipe() for _ in range(nenvs)])\n",
        "        self.ps = [mp.Process(target=worker, args=(work_remote, remote, CloudpickleWrapper(env_fn)))\n",
        "            for (work_remote, remote, env_fn) in zip(self.work_remotes, self.remotes, env_fns)]\n",
        "        for p in self.ps:\n",
        "            p.daemon = True # if the main process crashes, we should not cause things to hang\n",
        "            p.start()\n",
        "        for remote in self.work_remotes:\n",
        "            remote.close()\n",
        "\n",
        "        self.remotes[0].send(('get_spaces', None))\n",
        "        observation_space, action_space = self.remotes[0].recv()\n",
        "        VecEnv.__init__(self, len(env_fns), observation_space, action_space)\n",
        "\n",
        "    def step_async(self, actions):\n",
        "        if(type(actions) == int):\n",
        "            for remote in self.remotes:\n",
        "                remote.send(('step', actions))\n",
        "        else:\n",
        "            for remote, action in zip(self.remotes, actions):\n",
        "                remote.send(('step', action))\n",
        "        self.waiting = True\n",
        "\n",
        "    def step_wait(self):\n",
        "        results = [remote.recv() for remote in self.remotes]\n",
        "        self.waiting = False\n",
        "        obs, rews, dones, infos = zip(*results)\n",
        "        return np.stack(obs), np.stack(rews), np.stack(dones), infos\n",
        "\n",
        "    def reset(self):\n",
        "        for remote in self.remotes:\n",
        "            remote.send(('reset', None))\n",
        "        return np.stack([remote.recv() for remote in self.remotes])\n",
        "\n",
        "    def reset_task(self):\n",
        "        for remote in self.remotes:\n",
        "            remote.send(('reset_task', None))\n",
        "        return np.stack([remote.recv() for remote in self.remotes])\n",
        "\n",
        "    def close(self):\n",
        "        if self.closed:\n",
        "            return\n",
        "        if self.waiting:\n",
        "            for remote in self.remotes:\n",
        "                remote.recv()\n",
        "        for remote in self.remotes:\n",
        "            remote.send(('close', None))\n",
        "        for p in self.ps:\n",
        "            p.join()\n",
        "            self.closed = True\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.nenvs\n",
        "\n",
        "\n",
        "\n",
        "def shape_list(x):\n",
        "  \"\"\"Return list of dims, statically where possible.\"\"\"\n",
        "  x = tf.convert_to_tensor(x)\n",
        "\n",
        "  # If unknown rank, return dynamic shape\n",
        "  if x.get_shape().dims is None:\n",
        "    return tf.shape(x)\n",
        "\n",
        "  static = x.get_shape().as_list()\n",
        "  shape = tf.shape(x)\n",
        "\n",
        "  ret = []\n",
        "  for i, dim in enumerate(static):\n",
        "    if dim is None:\n",
        "      dim = shape[i]\n",
        "    ret.append(dim)\n",
        "  return ret\n",
        "\n",
        "def to_float(x):\n",
        "  \"\"\"Cast x to float; created because tf.to_float is deprecated.\"\"\"\n",
        "  return tf.cast(x, tf.float32)\n",
        "\n",
        "def cast_like(x, y):\n",
        "  \"\"\"Cast x to y's dtype, if necessary.\"\"\"\n",
        "  x = tf.convert_to_tensor(x)\n",
        "  y = tf.convert_to_tensor(y)\n",
        "\n",
        "  if x.dtype.base_dtype == y.dtype.base_dtype:\n",
        "    return x\n",
        "\n",
        "  cast_x = tf.cast(x, y.dtype)\n",
        "  if cast_x.device != x.device:\n",
        "    x_name = \"(eager Tensor)\"\n",
        "    try:\n",
        "      x_name = x.name\n",
        "    except AttributeError:\n",
        "      pass\n",
        "    tf.logging.warning(\"Cast for %s may induce copy from '%s' to '%s'\", x_name,\n",
        "                       x.device, cast_x.device)\n",
        "  return cast_x\n",
        "\n",
        "def layer_norm_vars(filters):\n",
        "  \"\"\"Create Variables for layer norm.\"\"\"\n",
        "  scale = tf.get_variable(\n",
        "      \"layer_norm_scale\", [filters], initializer=tf.ones_initializer())\n",
        "  bias = tf.get_variable(\n",
        "      \"layer_norm_bias\", [filters], initializer=tf.zeros_initializer())\n",
        "  return scale, bias\n",
        "\n",
        "\n",
        "def layer_norm_compute(x, epsilon, scale, bias, layer_collection=None):\n",
        "  \"\"\"Layer norm raw computation.\"\"\"\n",
        "\n",
        "  # Save these before they get converted to tensors by the casting below\n",
        "  params = (scale, bias)\n",
        "\n",
        "  epsilon, scale, bias = [cast_like(t, x) for t in [epsilon, scale, bias]]\n",
        "  mean = tf.reduce_mean(x, axis=[-1], keepdims=True)\n",
        "  variance = tf.reduce_mean(\n",
        "      tf.squared_difference(x, mean), axis=[-1], keepdims=True)\n",
        "  norm_x = (x - mean) * tf.rsqrt(variance + epsilon)\n",
        "\n",
        "  output = norm_x * scale + bias\n",
        "\n",
        "\n",
        "  return output\n",
        "\n",
        "def layer_norm(x,\n",
        "               filters=None,\n",
        "               epsilon=1e-6,\n",
        "               name=None,\n",
        "               reuse=None,\n",
        "               layer_collection=None):\n",
        "  \"\"\"Layer normalize the tensor x, averaging over the last dimension.\"\"\"\n",
        "  if filters is None:\n",
        "    filters = shape_list(x)[-1]\n",
        "  with tf.variable_scope(\n",
        "      name, default_name=\"layer_norm\", values=[x], reuse=reuse):\n",
        "    scale, bias = layer_norm_vars(filters)\n",
        "    return layer_norm_compute(x, epsilon, scale, bias,\n",
        "                              layer_collection=layer_collection)\n",
        "\n",
        "def standardize_images(x):\n",
        "  \"\"\"Image standardization on batches and videos.\"\"\"\n",
        "  with tf.name_scope(\"standardize_images\", values=[x]):\n",
        "    x_shape = shape_list(x)\n",
        "    x = to_float(tf.reshape(x, [-1] + x_shape[-3:]))\n",
        "    x_mean = tf.reduce_mean(x, axis=[1, 2], keepdims=True)\n",
        "    x_variance = tf.reduce_mean(\n",
        "        tf.squared_difference(x, x_mean), axis=[1, 2], keepdims=True)\n",
        "    num_pixels = to_float(x_shape[-2] * x_shape[-3])\n",
        "    x = (x - x_mean) / tf.maximum(tf.sqrt(x_variance), tf.rsqrt(num_pixels))\n",
        "    return tf.reshape(x, x_shape)\n",
        "\n",
        "def pad_to_same_length(x, y, final_length_divisible_by=1, axis=1):\n",
        "  \"\"\"Pad tensors x and y on axis 1 so that they have the same length.\"\"\"\n",
        "  if axis not in [1, 2]:\n",
        "    raise ValueError(\"Only axis=1 and axis=2 supported for now.\")\n",
        "  with tf.name_scope(\"pad_to_same_length\", values=[x, y]):\n",
        "    x_length = shape_list(x)[axis]\n",
        "    y_length = shape_list(y)[axis]\n",
        "    if (isinstance(x_length, int) and isinstance(y_length, int) and\n",
        "        x_length == y_length and final_length_divisible_by == 1):\n",
        "      return x, y\n",
        "    max_length = tf.maximum(x_length, y_length)\n",
        "    if final_length_divisible_by > 1:\n",
        "      # Find the nearest larger-or-equal integer divisible by given number.\n",
        "      max_length += final_length_divisible_by - 1\n",
        "      max_length //= final_length_divisible_by\n",
        "      max_length *= final_length_divisible_by\n",
        "    length_diff1 = max_length - x_length\n",
        "    length_diff2 = max_length - y_length\n",
        "\n",
        "    def padding_list(length_diff, arg):\n",
        "      if axis == 1:\n",
        "        return [[[0, 0], [0, length_diff]],\n",
        "                tf.zeros([tf.rank(arg) - 2, 2], dtype=tf.int32)]\n",
        "      return [[[0, 0], [0, 0], [0, length_diff]],\n",
        "              tf.zeros([tf.rank(arg) - 3, 2], dtype=tf.int32)]\n",
        "\n",
        "    paddings1 = tf.concat(padding_list(length_diff1, x), axis=0)\n",
        "    paddings2 = tf.concat(padding_list(length_diff2, y), axis=0)\n",
        "    res_x = tf.pad(x, paddings1)\n",
        "    res_y = tf.pad(y, paddings2)\n",
        "    # Static shapes are the same except for axis=1.\n",
        "    x_shape = x.shape.as_list()\n",
        "    x_shape[axis] = None\n",
        "    res_x.set_shape(x_shape)\n",
        "    y_shape = y.shape.as_list()\n",
        "    y_shape[axis] = None\n",
        "    res_y.set_shape(y_shape)\n",
        "    return res_x, res_y\n",
        "\n",
        "def make_even_size(x):\n",
        "  \"\"\"Pad x to be even-sized on axis 1 and 2, but only if necessary.\"\"\"\n",
        "  x_shape = x.get_shape().as_list()\n",
        "  assert len(x_shape) > 2, \"Only 3+-dimensional tensors supported.\"\n",
        "  shape = [dim if dim is not None else -1 for dim in x_shape]\n",
        "  new_shape = x_shape  # To make sure constant shapes remain constant.\n",
        "  if x_shape[1] is not None:\n",
        "    new_shape[1] = 2 * int(math.ceil(x_shape[1] * 0.5))\n",
        "  if x_shape[2] is not None:\n",
        "    new_shape[2] = 2 * int(math.ceil(x_shape[2] * 0.5))\n",
        "  if shape[1] % 2 == 0 and shape[2] % 2 == 0:\n",
        "    return x\n",
        "  if shape[1] % 2 == 0:\n",
        "    x, _ = pad_to_same_length(x, x, final_length_divisible_by=2, axis=2)\n",
        "    x.set_shape(new_shape)\n",
        "    return x\n",
        "  if shape[2] % 2 == 0:\n",
        "    x, _ = pad_to_same_length(x, x, final_length_divisible_by=2, axis=1)\n",
        "    x.set_shape(new_shape)\n",
        "    return x\n",
        "  x, _ = pad_to_same_length(x, x, final_length_divisible_by=2, axis=1)\n",
        "  x, _ = pad_to_same_length(x, x, final_length_divisible_by=2, axis=2)\n",
        "  x.set_shape(new_shape)\n",
        "  return x\n",
        "\n",
        "\n",
        "def add_timing_signal_nd(x, min_timescale=1.0, max_timescale=1.0e4):\n",
        "  \"\"\"Adds a bunch of sinusoids of different frequencies to a Tensor.\n",
        "\n",
        "  Each channel of the input Tensor is incremented by a sinusoid of a different\n",
        "  frequency and phase in one of the positional dimensions.\n",
        "\n",
        "  This allows attention to learn to use absolute and relative positions.\n",
        "  Timing signals should be added to some precursors of both the query and the\n",
        "  memory inputs to attention.\n",
        "\n",
        "  The use of relative position is possible because sin(a+b) and cos(a+b) can be\n",
        "  expressed in terms of b, sin(a) and cos(a).\n",
        "\n",
        "  x is a Tensor with n \"positional\" dimensions, e.g. one dimension for a\n",
        "  sequence or two dimensions for an image\n",
        "\n",
        "  We use a geometric sequence of timescales starting with\n",
        "  min_timescale and ending with max_timescale.  The number of different\n",
        "  timescales is equal to channels // (n * 2). For each timescale, we\n",
        "  generate the two sinusoidal signals sin(timestep/timescale) and\n",
        "  cos(timestep/timescale).  All of these sinusoids are concatenated in\n",
        "  the channels dimension.\n",
        "\n",
        "  Args:\n",
        "    x: a Tensor with shape [batch, d1 ... dn, channels]\n",
        "    min_timescale: a float\n",
        "    max_timescale: a float\n",
        "\n",
        "  Returns:\n",
        "    a Tensor the same shape as x.\n",
        "  \"\"\"\n",
        "  num_dims = len(x.get_shape().as_list()) - 2\n",
        "  channels = shape_list(x)[-1]\n",
        "  num_timescales = channels // (num_dims * 2)\n",
        "  log_timescale_increment = (\n",
        "      math.log(float(max_timescale) / float(min_timescale)) /\n",
        "      (tf.to_float(num_timescales) - 1))\n",
        "  inv_timescales = min_timescale * tf.exp(\n",
        "      tf.to_float(tf.range(num_timescales)) * -log_timescale_increment)\n",
        "  for dim in range(num_dims):\n",
        "    length = shape_list(x)[dim + 1]\n",
        "    position = tf.to_float(tf.range(length))\n",
        "    scaled_time = tf.expand_dims(position, 1) * tf.expand_dims(\n",
        "        inv_timescales, 0)\n",
        "    signal = tf.concat([tf.sin(scaled_time), tf.cos(scaled_time)], axis=1)\n",
        "    prepad = dim * 2 * num_timescales\n",
        "    postpad = channels - (dim + 1) * 2 * num_timescales\n",
        "    signal = tf.pad(signal, [[0, 0], [prepad, postpad]])\n",
        "    for _ in range(1 + dim):\n",
        "      signal = tf.expand_dims(signal, 0)\n",
        "    for _ in range(num_dims - 1 - dim):\n",
        "      signal = tf.expand_dims(signal, -2)\n",
        "    x += signal\n",
        "  return x\n"
      ],
      "metadata": {
        "id": "jI5IAwFanTBW"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "g_env_model = None\n",
        "\n",
        "def cached_world_model(ob_shape, action_dim, config, path):\n",
        "    global g_env_model\n",
        "    if g_env_model is None:\n",
        "        old_val = config.n_envs\n",
        "        config.n_envs = 1\n",
        "        g_env_model = EnvModel(ob_shape, action_dim, config)\n",
        "        g_env_model.load_state_dict(torch.load(path))\n",
        "        g_env_model.eval()\n",
        "        printstar('World Model Restored')\n",
        "        config.n_envs = old_val\n",
        "\n",
        "    return g_env_model\n",
        "\n",
        "class EnvModel(nn.Module):\n",
        "    def __init__(self, obs_shape, action_dim, config):\n",
        "        super(EnvModel, self).__init__()\n",
        "        self.obs_shape = obs_shape\n",
        "        self.action_dim = action_dim\n",
        "        self.config = config\n",
        "\n",
        "        self.hidden_size = config.hidden_size\n",
        "        self.layers = config.n_layers\n",
        "        self.dropout_p = config.dropout_p\n",
        "\n",
        "        self.activation_fn = nn.ReLU() if config.activation_fn == 'relu' else nn.Tanh()\n",
        "\n",
        "        self.l2_clip = config.l2_clip\n",
        "        self.softmax_clip = config.softmax_clip\n",
        "        self.reward_coeff = config.reward_coeff\n",
        "        self.n_envs = config.n_envs\n",
        "        self.max_ep_len = config.max_ep_len\n",
        "        self.log_interval = config.log_interval\n",
        "\n",
        "        self.has_rewards = config.has_rewards\n",
        "        self.num_rewards = config.num_rewards\n",
        "\n",
        "        self.width, self.height, self.depth = self.obs_shape\n",
        "\n",
        "        # Define model architecture\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(self.depth, self.hidden_size, kernel_size=3, stride=1, padding=1),\n",
        "            self.activation_fn,\n",
        "            nn.Dropout(self.dropout_p)\n",
        "        )\n",
        "        self.middle_layers = nn.ModuleList([\n",
        "            nn.Sequential(\n",
        "                nn.Conv2d(self.hidden_size, self.hidden_size, kernel_size=3, stride=1, padding=1),\n",
        "                self.activation_fn,\n",
        "                nn.Dropout(self.dropout_p)\n",
        "            ) for _ in range(self.layers)\n",
        "        ])\n",
        "        self.decoder = nn.Conv2d(self.hidden_size, self.depth, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "        self.reward_predictor = None\n",
        "        if self.has_rewards:\n",
        "            self.reward_predictor = nn.Sequential(\n",
        "                nn.Linear(self.hidden_size, 128),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(128, self.num_rewards)\n",
        "            )\n",
        "\n",
        "    def forward(self, obs, action):\n",
        "        # Standardize images\n",
        "        obs = standardize_images(obs)\n",
        "        x = self.encoder(obs)\n",
        "        for layer in self.middle_layers:\n",
        "            x = x + layer(x)\n",
        "        state_pred = self.decoder(x)\n",
        "\n",
        "        reward_pred = None\n",
        "        if self.has_rewards:\n",
        "            x_flat = torch.mean(x, dim=[2, 3])\n",
        "            reward_pred = self.reward_predictor(x_flat)\n",
        "\n",
        "        return state_pred, reward_pred\n",
        "\n",
        "    def imagine(self, obs, action):\n",
        "        obs = obs.unsqueeze(0).permute(0, 3, 1, 2)  # Add batch dimension, switch to (B, C, H, W)\n",
        "        action = torch.tensor(action, dtype=torch.float32).unsqueeze(0)\n",
        "        with torch.no_grad():\n",
        "            next_pred_ob, _ = self.forward(obs, action)\n",
        "        return next_pred_ob.squeeze(0).permute(1, 2, 0).numpy()  # Back to (H, W, C)\n",
        "\n",
        "    def train_model(self, envs, config, world_model_path):\n",
        "        optimizer = optim.Adam(self.parameters(), lr=0.001)\n",
        "        loss_fn = nn.MSELoss()\n",
        "\n",
        "        writer = SummaryWriter(log_dir='./env_logs/train')\n",
        "\n",
        "        for idx, states, actions, rewards, next_states, dones in tqdm(\n",
        "            self.generate_data(envs), total=self.max_ep_len):\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Convert inputs to tensors\n",
        "            states = torch.tensor(states, dtype=torch.float32).permute(0, 3, 1, 2)\n",
        "            actions = torch.tensor(actions, dtype=torch.float32)\n",
        "            next_states = torch.tensor(next_states, dtype=torch.float32).permute(0, 3, 1, 2)\n",
        "\n",
        "            state_pred, reward_pred = self.forward(states, actions)\n",
        "\n",
        "            state_loss = torch.clamp(loss_fn(state_pred, next_states), max=self.l2_clip)\n",
        "\n",
        "            loss = state_loss\n",
        "            if self.has_rewards:\n",
        "                rewards = torch.tensor(rewards, dtype=torch.float32)\n",
        "                reward_loss = torch.clamp(loss_fn(reward_pred, rewards), max=self.softmax_clip)\n",
        "                loss += self.reward_coeff * reward_loss\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            if idx % config.log_interval == 0:\n",
        "                print(f'{idx} => Loss: {loss.item():.4f}')\n",
        "                torch.save(self.state_dict(), os.path.join(world_model_path, 'env_model.pt'))\n",
        "                print('Environment model saved')\n",
        "\n",
        "                writer.add_scalar('Loss/Total', loss.item(), idx)\n",
        "        writer.close()\n",
        "\n",
        "    def generate_data(self, envs):\n",
        "        states = envs.reset()\n",
        "        for frame_idx in range(self.max_ep_len):\n",
        "            states = states.reshape(self.n_envs, self.width, self.height, self.depth)\n",
        "            actions = [envs.action_space.sample() for _ in range(self.n_envs)]\n",
        "            next_states, rewards, dones, _ = envs.step(actions)\n",
        "            next_states = next_states.reshape(self.n_envs, self.width, self.height, self.depth)\n",
        "\n",
        "            yield frame_idx, states, actions, rewards, next_states, dones\n",
        "            states = next_states\n",
        "            if self.n_envs == 1 and dones:\n",
        "                states = envs.reset()\n"
      ],
      "metadata": {
        "id": "KnRlZx-Cn4SZ"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "g_env_model = None\n",
        "\n",
        "def main(config):\n",
        "    global make_env\n",
        "    env = make_env()()\n",
        "    env.reset()\n",
        "\n",
        "    action_dim = env.action_space.shape[0]  # Assuming continuous action space\n",
        "    ob_shape = env.observation_space.shape\n",
        "    world_model_path = os.path.expanduser(os.path.join(config.model_dir, config.world_model_path))\n",
        "\n",
        "    env_model = EnvModel(ob_shape, action_dim, config)\n",
        "    optimizer = optim.Adam(env_model.parameters(), lr=0.001)\n",
        "\n",
        "    if config.train_world_model:\n",
        "        if not os.path.exists(world_model_path):\n",
        "            os.makedirs(world_model_path)\n",
        "        printstar(\"Training World Model\")\n",
        "        env_model.train_model(env, config, optimizer)\n",
        "        torch.save(env_model.state_dict(), os.path.join(world_model_path, 'env_model.pt'))\n",
        "\n",
        "    if config.eval_world_model:\n",
        "        env_model.load_state_dict(torch.load(os.path.join(world_model_path, 'env_model.pt')))\n",
        "        evaluate_world_model(env, env_model, config)\n",
        "\n",
        "def evaluate_world_model(env, env_model, config, policy=None):\n",
        "    printstar(\"Testing World Model\")\n",
        "    env_model.eval()\n",
        "    obs = env.reset()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for t in range(config.max_eval_iters):\n",
        "            action = policy(obs) if policy else env.action_space.sample()\n",
        "            action_tensor = torch.tensor(action, dtype=torch.float32).unsqueeze(0)\n",
        "            obs_tensor = torch.tensor(obs, dtype=torch.float32).unsqueeze(0)\n",
        "\n",
        "            next_pred_ob = env_model(obs_tensor, action_tensor).squeeze(0).numpy()\n",
        "            plt.imshow(next_pred_ob.reshape(env.observation_space.shape[:2]))\n",
        "            plt.savefig('./figs/world_model_eval.png')\n",
        "\n",
        "            env.render()\n",
        "            obs, _, done, _ = env.step(action)\n",
        "            if input(\"Press 0 to exit: \") == \"0\":\n",
        "                break\n",
        "            if done:\n",
        "                obs = env.reset()\n",
        "\n",
        "# Configure and run in the notebook\n",
        "if __name__ == '__main__':\n",
        "    config = Config(train_world_model=True, eval_world_model=True, debug=False)  # Update as needed\n",
        "    main(config)\n"
      ],
      "metadata": {
        "id": "dO2QgbsdnJq6",
        "outputId": "21f9beac-e6a5-4039-8d72-bc7d6e30ced9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        }
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "**************************************************\n",
            "Training World Model\n",
            "**************************************************\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "SummaryWriter() takes no arguments",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-61-75305d69a8a9>\u001b[0m in \u001b[0;36m<cell line: 49>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_world_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_world_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdebug\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Update as needed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'CUDA_VISIBLE_DEVICES'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m  \u001b[0;31m# Optional: Disable GPU for testing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-61-75305d69a8a9>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mworld_model_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mprintstar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training World Model\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0menv_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mworld_model_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'env_model.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-60-a2e69f586ecd>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(self, envs, config, world_model_path)\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0mloss_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMSELoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         \u001b[0mwriter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSummaryWriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'./env_logs/train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         for idx, states, actions, rewards, next_states, dones in tqdm(\n",
            "\u001b[0;31mTypeError\u001b[0m: SummaryWriter() takes no arguments"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# main.py\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from world_model import *\n",
        "from config import argparser\n",
        "from utils import make_env as env_fn, printstar\n",
        "\n",
        "g_env_model = None\n",
        "\n",
        "def main(config):\n",
        "    global env_fn\n",
        "    env = env_fn()()\n",
        "    env.reset()\n",
        "\n",
        "    action_dim = 5\n",
        "    ob_shape = env.observation_space.shape\n",
        "    world_model_path = os.path.expanduser(os.path.join(config.model_dir, config.world_model_type + \"_\" + config.world_model_path))\n",
        "\n",
        "    if(config.train_world_model):\n",
        "        env_model = EnvModel(ob_shape, action_dim, config)\n",
        "        if(not os.path.exists(world_model_path)):\n",
        "            os.mkdir(world_model_path)\n",
        "        printstar(\"Training World Model\")\n",
        "        env_model.train(world_model_path)\n",
        "\n",
        "    with tf.Session() as sess:\n",
        "        sess.run(tf.global_variables_initializer())\n",
        "        if(config.eval_world_model):\n",
        "            env_model = cached_world_model(sess, ob_shape, action_dim, config, world_model_path + '/env_model.ckpt')\n",
        "            evaluate_world_model(env, sess, env_model, config)\n",
        "\n",
        "def evaluate_world_model(env, sess, env_model, config, policy=None):\n",
        "    printstar(\"Testing World Model\")\n",
        "    obs = env.reset()\n",
        "    for t in range(config.max_eval_iters):\n",
        "        if(policy is None):\n",
        "                action = env.action_space.sample()\n",
        "        else:\n",
        "            action = policy(obs)\n",
        "\n",
        "        next_pred_ob = env_model.imagine(sess, obs, action)\n",
        "        imgplot = plt.imshow(next_pred_ob)\n",
        "        plt.savefig('./figs/world_model_eval.png')\n",
        "\n",
        "        env.render()\n",
        "        obs, reward, dones, info = env.step(action)\n",
        "        inp = input(\"Press 0 to exit : \")\n",
        "        if(inp == \"0\"):\n",
        "            break\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    config = argparser()\n",
        "    mp.set_start_method('spawn', force=True)\n",
        "    main(config)"
      ],
      "metadata": {
        "id": "BDSlARzOmwiB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def collect_data(env, num_episodes=1000):\n",
        "    data = []\n",
        "    for _ in range(num_episodes):\n",
        "        state = env.reset()\n",
        "        terminated = False\n",
        "        truncated = False\n",
        "        while not (terminated or truncated):\n",
        "            action = env.action_space.sample()\n",
        "            next_state, reward, terminated, truncated, info = env.step(action)\n",
        "            data.append((state, action, reward, next_state))\n",
        "            if(terminated or truncated):\n",
        "              print(next_state)\n",
        "            state = next_state\n",
        "    return data\n",
        "\n",
        "# Collect data\n",
        "\n",
        "data = collect_data(env)\n",
        "print(len(data))"
      ],
      "metadata": {
        "id": "GYPDC21YuHMR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(data[0][0])"
      ],
      "metadata": {
        "id": "AJaLKHvpwsUv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Define the neural network for the dynamics model\n",
        "class DynamicsModel(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim):\n",
        "        super(DynamicsModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(state_dim + action_dim, 128)\n",
        "        self.fc2 = nn.Linear(128, 128)\n",
        "        self.fc3 = nn.Linear(128, state_dim)  # Predict next state\n",
        "        self.reward = nn.Linear(128, 1)       # Predict reward\n",
        "\n",
        "    def forward(self, state, action):\n",
        "        x = torch.cat([state, action], dim=-1)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        next_state = self.fc3(x)\n",
        "        reward = self.reward(x)\n",
        "        return next_state, reward\n"
      ],
      "metadata": {
        "id": "3fVjGrdzuJ4-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize model and optimizer\n",
        "state_dim = env.observation_space.shape[0]\n",
        "action_dim = env.action_space.n\n",
        "model = DynamicsModel(state_dim, action_dim)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "# Convert collected data to tensors for training\n",
        "states = torch.tensor([d[0] for d in data], dtype=torch.float32)\n",
        "actions = torch.tensor([d[1] for d in data], dtype=torch.float32).unsqueeze(1)\n",
        "next_states = torch.tensor([d[3] for d in data], dtype=torch.float32)\n",
        "rewards = torch.tensor([d[2] for d in data], dtype=torch.float32).unsqueeze(1)\n",
        "\n",
        "# Train the dynamics model\n",
        "for epoch in range(100):\n",
        "    optimizer.zero_grad()\n",
        "    predicted_next_states, predicted_rewards = model(states, actions)\n",
        "    loss = criterion(predicted_next_states, next_states) + criterion(predicted_rewards, rewards)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "print(\"Model training complete!\")\n"
      ],
      "metadata": {
        "id": "ja_lVS2RuhL0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "[d[0] for d in data]"
      ],
      "metadata": {
        "id": "k33oyoqkw62R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mpc_action_selection(model, current_state, num_simulations=100, horizon=10):\n",
        "    best_action = None\n",
        "    best_reward = -np.inf\n",
        "\n",
        "    for _ in range(num_simulations):\n",
        "        simulated_state = current_state\n",
        "        total_reward = 0\n",
        "        for _ in range(horizon):\n",
        "            action = np.random.choice([0, 1])  # Random action sampling for now\n",
        "            action_tensor = torch.tensor([action], dtype=torch.float32).unsqueeze(0)\n",
        "            state_tensor = torch.tensor(simulated_state, dtype=torch.float32).unsqueeze(0)\n",
        "            next_state, reward = model(state_tensor, action_tensor)\n",
        "            total_reward += reward.item()\n",
        "            simulated_state = next_state.detach().numpy()[0]\n",
        "\n",
        "        if total_reward > best_reward:\n",
        "            best_reward = total_reward\n",
        "            best_action = action\n",
        "\n",
        "    return best_action\n"
      ],
      "metadata": {
        "id": "MSExKxdfui3I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model_based_agent(env, model, num_episodes=10):\n",
        "    for episode in range(num_episodes):\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "        total_reward = 0\n",
        "        while not done:\n",
        "            action = mpc_action_selection(model, state)\n",
        "            state, reward, done, _ = env.step(action)\n",
        "            total_reward += reward\n",
        "        print(f\"Episode {episode + 1}: Total Reward: {total_reward}\")\n",
        "\n",
        "# Evaluate the agent\n",
        "evaluate_model_based_agent(env, model)\n"
      ],
      "metadata": {
        "id": "thkJAs5LukYY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# world_model.py\n",
        "\n",
        "import os\n",
        "import gym\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import tensorflow as tf\n",
        "\n",
        "from utils import *\n",
        "\n",
        "g_env_model = None\n",
        "def cached_world_model(sess, ob_shape, action_dim, config, path):\n",
        "    global g_env_model\n",
        "    if g_env_model is None:\n",
        "        old_val = config.n_envs\n",
        "        config.n_envs = 1\n",
        "        g_env_model = EnvModel(ob_shape, action_dim, config)\n",
        "        save_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='env_model')\n",
        "        loader = tf.train.Saver(var_list=save_vars)\n",
        "        loader.restore(sess, path)\n",
        "        printstar('World Model Restored')\n",
        "        config.n_envs = old_val\n",
        "\n",
        "    return g_env_model\n",
        "\n",
        "def inject_additional_input(layer, inputs, name, mode=\"multi_additive\"):\n",
        "  \"\"\"Injects the additional input into the layer.\n",
        "\n",
        "  Args:\n",
        "    layer: layer that the input should be injected to.\n",
        "    inputs: inputs to be injected.\n",
        "    name: TF scope name.\n",
        "    mode: how the infor should be added to the layer:\n",
        "      \"concat\" concats as additional channels.\n",
        "      \"multiplicative\" broadcasts inputs and multiply them to the channels.\n",
        "      \"multi_additive\" broadcasts inputs and multiply and add to the channels.\n",
        "\n",
        "  Returns:\n",
        "    updated layer.\n",
        "\n",
        "  Raises:\n",
        "    ValueError: in case of unknown mode.\n",
        "  \"\"\"\n",
        "  layer_shape = shape_list(layer)\n",
        "  input_shape = shape_list(inputs)\n",
        "  zeros_mask = tf.zeros(layer_shape, dtype=tf.float32)\n",
        "  if mode == \"concat\":\n",
        "    emb = common_video.encode_to_shape(inputs, layer_shape, name)\n",
        "    layer = tf.concat(values=[layer, emb], axis=-1)\n",
        "  elif mode == \"multiplicative\":\n",
        "    filters = layer_shape[-1]\n",
        "    input_reshaped = tf.reshape(inputs, [-1, 1, 1, input_shape[-1]])\n",
        "    input_mask = tf.layers.dense(input_reshaped, filters, name=name)\n",
        "    input_broad = input_mask + zeros_mask\n",
        "    layer *= input_broad\n",
        "  elif mode == \"multi_additive\":\n",
        "    filters = layer_shape[-1]\n",
        "    input_reshaped = tf.reshape(inputs, [-1, 1, 1, input_shape[-1]])\n",
        "    input_mul = tf.layers.dense(input_reshaped, filters, name=name + \"_mul\")\n",
        "    layer *= tf.nn.sigmoid(input_mul)\n",
        "    input_add = tf.layers.dense(input_reshaped, filters, name=name + \"_add\")\n",
        "    layer += input_add\n",
        "  else:\n",
        "    raise ValueError(\"Unknown injection mode: %s\" % mode)\n",
        "\n",
        "  return layer\n",
        "\n",
        "class EnvModel(object):\n",
        "    def __init__(self, obs_shape, action_dim, config):\n",
        "\n",
        "        self.obs_shape = obs_shape\n",
        "        self.action_dim = action_dim\n",
        "        self.config = config\n",
        "\n",
        "        self.hidden_size = config.hidden_size\n",
        "        self.layers = config.n_layers\n",
        "        self.dropout_p = config.dropout_p\n",
        "\n",
        "        if(config.activation_fn == 'relu'):\n",
        "            self.activation_fn = tf.nn.relu\n",
        "        elif (config.activation_fn == 'tanh'):\n",
        "            self.activation_fn = tf.nn.tanh\n",
        "\n",
        "        self.l2_clip = config.l2_clip\n",
        "        self.softmax_clip = config.softmax_clip\n",
        "        self.reward_coeff = config.reward_coeff\n",
        "        self.n_envs = config.n_envs\n",
        "        self.max_ep_len = config.max_ep_len\n",
        "        self.log_interval = config.log_interval\n",
        "\n",
        "        self.is_policy = config.is_policy\n",
        "        self.has_rewards = config.has_rewards\n",
        "        self.num_rewards = config.num_rewards\n",
        "\n",
        "        self.width, self.height, self.depth = self.obs_shape\n",
        "\n",
        "        self.states_ph = tf.placeholder(tf.float32, [None, self.width, self.height, self.depth])\n",
        "        self.actions_ph = tf.placeholder(tf.uint8, [None, 1])\n",
        "        self.actions_oph = tf.one_hot(self.actions_ph, depth=action_dim)\n",
        "        self.target_states = tf.placeholder(tf.float32, [None, self.width, self.height, self.depth])\n",
        "        if(self.has_rewards):\n",
        "            self.target_rewards = tf.placeholder(tf.uint8, [None, self.num_rewards])\n",
        "\n",
        "        # NOTE - Implement policy and value parts later\n",
        "        with tf.variable_scope(\"env_model\"):\n",
        "            self.state_pred, self.reward_pred, _, _ = self.network()\n",
        "\n",
        "        # NOTE - Change this maybe to video_l2_loss\n",
        "        self.state_loss = tf.math.maximum(tf.reduce_sum(tf.pow(self.state_pred - self.target_states, 2)), self.l2_clip)\n",
        "        self.loss = self.state_loss\n",
        "\n",
        "        if(self.has_rewards):\n",
        "            self.reward_loss = tf.math.maximum(tf.reduce_mean(tf.losses.softmax_cross_entropy(self.tw_one_hot, self.reward_pred)), self.softmax_clip)\n",
        "            self.loss = self.loss + (self.reward_coeff * self.reward_loss)\n",
        "\n",
        "        self.opt = tf.train.AdamOptimizer().minimize(self.loss)\n",
        "\n",
        "        tf.summary.scalar('loss', self.loss)\n",
        "        if(self.has_rewards):\n",
        "            tf.summary.scalar('image_loss', self.state_loss)\n",
        "            tf.summary.scalar('reward_loss', self.reward_loss)\n",
        "\n",
        "    def generate_data(self, envs):\n",
        "        states = envs.reset()\n",
        "        for frame_idx in range(self.max_ep_len):\n",
        "            states = states.reshape(self.n_envs, self.width, self.height, self.depth)\n",
        "            if(self.n_envs == 1):\n",
        "                actions = envs.action_space.sample()\n",
        "            else:\n",
        "                actions = [envs.action_space.sample() for _ in range(self.n_envs)]\n",
        "            next_states, rewards, dones, _ = envs.step(actions)\n",
        "            next_states = next_states.reshape(self.n_envs, self.width, self.height, self.depth)\n",
        "\n",
        "            yield frame_idx, states, actions, rewards, next_states, dones\n",
        "            states = next_states\n",
        "            if(self.n_envs == 1 and dones == True):\n",
        "                states = envs.reset()\n",
        "\n",
        "    def network(self):\n",
        "        def middle_network(layer):\n",
        "            x = layer\n",
        "            kernel1 = (3, 3)\n",
        "            filters = shape_list(x)[-1]\n",
        "            for i in range(2):\n",
        "              with tf.variable_scope(\"layer%d\" % i):\n",
        "                y = tf.nn.dropout(x, 1.0 - 0.5)\n",
        "                y = tf.layers.conv2d(y, filters, kernel1, activation=self.activation_fn,\n",
        "                                     strides=(1, 1), padding=\"SAME\")\n",
        "                if i == 0:\n",
        "                  x = y\n",
        "                else:\n",
        "                  x = layer_norm(x + y)\n",
        "            return x\n",
        "\n",
        "        batch_size = tf.shape(self.states_ph)[0]\n",
        "\n",
        "        filters = self.hidden_size\n",
        "        kernel2 = (4, 4)\n",
        "        action = self.actions_oph\n",
        "\n",
        "        # Normalize states\n",
        "        if(self.n_envs > 1):\n",
        "            states = [standardize_images(self.states_ph[i, :, :, :]) for i in range(self.n_envs)]\n",
        "            stacked_states = tf.stack(states)\n",
        "        else:\n",
        "            stacked_states = standardize_images(self.states_ph)\n",
        "        inputs_shape = shape_list(stacked_states)\n",
        "\n",
        "        # Using non-zero bias initializer below for edge cases of uniform inputs.\n",
        "        x = tf.layers.dense(\n",
        "            stacked_states, filters, name=\"inputs_embed\",\n",
        "            bias_initializer=tf.random_normal_initializer(stddev=0.01))\n",
        "        x = add_timing_signal_nd(x)\n",
        "\n",
        "        # Down-stride.\n",
        "        layer_inputs = [x]\n",
        "        for i in range(self.layers):\n",
        "          with tf.variable_scope(\"downstride%d\" % i):\n",
        "            layer_inputs.append(x)\n",
        "            x = tf.nn.dropout(x, 1.0 - self.dropout_p)\n",
        "            x = make_even_size(x)\n",
        "            if i < 2:\n",
        "              filters *= 2\n",
        "            x = add_timing_signal_nd(x)\n",
        "            x = tf.layers.conv2d(x, filters, kernel2, activation=self.activation_fn,\n",
        "                                 strides=(2, 2), padding=\"SAME\")\n",
        "            x = layer_norm(x)\n",
        "\n",
        "        if self.is_policy:\n",
        "          with tf.variable_scope(\"policy\"):\n",
        "            x_flat = tf.layers.flatten(x)\n",
        "            policy_pred = tf.layers.dense(x_flat, self.action_dim)\n",
        "            value_pred = tf.layers.dense(x_flat, 1)\n",
        "            value_pred = tf.squeeze(value_pred, axis=-1)\n",
        "        else:\n",
        "          policy_pred, value_pred = None, None\n",
        "\n",
        "        x = inject_additional_input(x, action, \"action_enc\", \"multi_additive\")\n",
        "\n",
        "        # Inject latent if present. Only for stochastic models.\n",
        "        target_states = standardize_images(self.target_states)\n",
        "\n",
        "        x_mid = tf.reduce_mean(x, axis=[1, 2], keepdims=True)\n",
        "        x = middle_network(x)\n",
        "\n",
        "        # Up-convolve.\n",
        "        layer_inputs = list(reversed(layer_inputs))\n",
        "        for i in range(self.layers):\n",
        "          with tf.variable_scope(\"upstride%d\" % i):\n",
        "            x = tf.nn.dropout(x, 1.0 - 0.1)\n",
        "            if i >= self.layers - 2:\n",
        "              filters //= 2\n",
        "            x = tf.layers.conv2d_transpose(\n",
        "                x, filters, kernel2, activation=self.activation_fn,\n",
        "                strides=(2, 2), padding=\"SAME\")\n",
        "            y = layer_inputs[i]\n",
        "            shape = shape_list(y)\n",
        "            x = x[:, :shape[1], :shape[2], :]\n",
        "            x = layer_norm(x + y)\n",
        "            x = add_timing_signal_nd(x)\n",
        "\n",
        "        # Cut down to original size.\n",
        "        x = x[:, :inputs_shape[1], :inputs_shape[2], :]\n",
        "        x_fin = tf.reduce_mean(x, axis=[1, 2], keepdims=True)\n",
        "\n",
        "        x = tf.layers.dense(x, self.depth, name=\"logits\")\n",
        "\n",
        "        reward_pred = None\n",
        "        if self.has_rewards:\n",
        "          # Reward prediction based on middle and final logits.\n",
        "          reward_pred = tf.concat([x_mid, x_fin], axis=-1)\n",
        "          reward_pred = tf.nn.relu(tf.layers.dense(\n",
        "              reward_pred, 128, name=\"reward_pred\"))\n",
        "          reward_pred = tf.squeeze(reward_pred, axis=1)  # Remove extra dims\n",
        "          reward_pred = tf.squeeze(reward_pred, axis=1)  # Remove extra dims\n",
        "\n",
        "        return x, reward_pred, policy_pred, value_pred\n",
        "\n",
        "    def imagine(self, sess, obs, action):\n",
        "        action = np.array(action)\n",
        "        action = np.reshape(action, (1, 1))\n",
        "        obs = obs.reshape(1, self.width, self.height, self.depth)\n",
        "        next_pred_ob = sess.run(self.state_pred, feed_dict={self.states_ph : obs, self.actions_ph : action})\n",
        "        next_pred_ob = next_pred_ob.reshape(self.width, self.height, self.depth)\n",
        "        next_pred_ob = np.rint(next_pred_ob)\n",
        "        return next_pred_ob\n",
        "\n",
        "    def train(self, world_model_path):\n",
        "        with tf.Session() as sess:\n",
        "            sess.run(tf.global_variables_initializer())\n",
        "\n",
        "            losses = []\n",
        "            all_rewards = []\n",
        "            save_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='env_model')\n",
        "            saver = tf.train.Saver(var_list=save_vars)\n",
        "\n",
        "            train_writer = tf.summary.FileWriter('./env_logs/train/', graph=sess.graph)\n",
        "            summary_op = tf.summary.merge_all()\n",
        "\n",
        "            if(self.n_envs == 1):\n",
        "                envs = make_env()()\n",
        "            else:\n",
        "                envs = [make_env() for i in range(self.n_envs)]\n",
        "                envs = SubprocVecEnv(envs)\n",
        "\n",
        "            for idx, states, actions, rewards, next_states, dones in tqdm(\n",
        "                self.generate_data(envs), total=self.max_ep_len):\n",
        "                actions = np.array(actions)\n",
        "                actions = np.reshape(actions, (-1, 1))\n",
        "\n",
        "                if(self.has_rewards):\n",
        "                    target_reward = reward_to_target(rewards)\n",
        "                    loss, reward_loss, state_loss, summary, _ = sess.run([self.loss, self.reward_loss, self.state_loss,\n",
        "                        summary_op, self.opt], feed_dict={\n",
        "                        self.states_ph: states,\n",
        "                        self.actions_ph: actions,\n",
        "                        self.target_states: next_states,\n",
        "                        self.target_rewards: target_reward\n",
        "                    })\n",
        "                else :\n",
        "                    loss, summary, _ = sess.run([self.loss, summary_op, self.opt], feed_dict={\n",
        "                        self.states_ph: states,\n",
        "                        self.actions_ph: actions,\n",
        "                        self.target_states: next_states,\n",
        "                    })\n",
        "\n",
        "                if idx % self.log_interval == 0:\n",
        "                    if(self.has_rewards):\n",
        "                        print('%i => Loss : %.4f, Reward Loss : %.4f, Image Loss : %.4f' % (idx, loss, reward_loss, state_loss))\n",
        "                    else :\n",
        "                        print('%i => Loss : %.4f' % (idx, loss))\n",
        "                    saver.save(sess, '{}/env_model.ckpt'.format(world_model_path))\n",
        "                    print('Environment model saved')\n",
        "\n",
        "                train_writer.add_summary(summary, idx)\n",
        "            envs.close()"
      ],
      "metadata": {
        "id": "UI5fQjGEeMqg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# main.py\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from world_model import *\n",
        "from config import argparser\n",
        "from utils import make_env as env_fn, printstar\n",
        "\n",
        "g_env_model = None\n",
        "\n",
        "def main(config):\n",
        "    global env_fn\n",
        "    env = env_fn()()\n",
        "    env.reset()\n",
        "\n",
        "    action_dim = 5\n",
        "    ob_shape = env.observation_space.shape\n",
        "    world_model_path = os.path.expanduser(os.path.join(config.model_dir, config.world_model_type + \"_\" + config.world_model_path))\n",
        "\n",
        "    if(config.train_world_model):\n",
        "        env_model = EnvModel(ob_shape, action_dim, config)\n",
        "        if(not os.path.exists(world_model_path)):\n",
        "            os.mkdir(world_model_path)\n",
        "        printstar(\"Training World Model\")\n",
        "        env_model.train(world_model_path)\n",
        "\n",
        "    with tf.Session() as sess:\n",
        "        sess.run(tf.global_variables_initializer())\n",
        "        if(config.eval_world_model):\n",
        "            env_model = cached_world_model(sess, ob_shape, action_dim, config, world_model_path + '/env_model.ckpt')\n",
        "            evaluate_world_model(env, sess, env_model, config)\n",
        "\n",
        "def evaluate_world_model(env, sess, env_model, config, policy=None):\n",
        "    printstar(\"Testing World Model\")\n",
        "    obs = env.reset()\n",
        "    for t in range(config.max_eval_iters):\n",
        "        if(policy is None):\n",
        "                action = env.action_space.sample()\n",
        "        else:\n",
        "            action = policy(obs)\n",
        "\n",
        "        next_pred_ob = env_model.imagine(sess, obs, action)\n",
        "        imgplot = plt.imshow(next_pred_ob)\n",
        "        plt.savefig('./figs/world_model_eval.png')\n",
        "\n",
        "        env.render()\n",
        "        obs, reward, dones, info = env.step(action)\n",
        "        inp = input(\"Press 0 to exit : \")\n",
        "        if(inp == \"0\"):\n",
        "            break\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    config = argparser()\n",
        "    mp.set_start_method('spawn', force=True)\n",
        "    main(config)"
      ],
      "metadata": {
        "id": "NmgPiaZcePVV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# config.py\n",
        "\n",
        "import os\n",
        "import argparse\n",
        "\n",
        "def str2bool(v):\n",
        "    return v.lower() == 'true'\n",
        "\n",
        "def str2list(v):\n",
        "    if not v:\n",
        "        return v\n",
        "    else:\n",
        "        return [v_ for v_ in v.split(',')]\n",
        "\n",
        "def argparser():\n",
        "    parser = argparse.ArgumentParser(\"Model-Based Reinforcement Learning for Atari\",\n",
        "                                     formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
        "\n",
        "    parser.add_argument('--train_world_model', type=str2bool, default=True)\n",
        "    parser.add_argument('--eval_world_model', type=str2bool, default=True)\n",
        "\n",
        "    parser.add_argument('--num_rewards', type=int, default=1)\n",
        "    parser.add_argument('--n_envs', type=int, default=16)\n",
        "    parser.add_argument('--is_policy', type=str2bool, default=False)\n",
        "    parser.add_argument('--has_rewards', type=str2bool, default=False)\n",
        "    parser.add_argument('--hidden_size', type=int, default=64)\n",
        "    parser.add_argument('--n_layers', type=int, default=6)\n",
        "    parser.add_argument('--dropout_p', type=float, default=0.1)\n",
        "    parser.add_argument('--max_ep_len', type=int, default=500000)\n",
        "    parser.add_argument('--l2_clip', type=float, default=0.0)\n",
        "    parser.add_argument('--softmax_clip', type=float, default=0.03)\n",
        "    parser.add_argument('--reward_coeff', type=float, default=0.1)\n",
        "    parser.add_argument('--log_interval', type=int, default=100)\n",
        "    parser.add_argument('--activation_fn', type=str, default='relu', choices=['relu', 'tanh'])\n",
        "\n",
        "    parser.add_argument('--log_dir', type=str, default='./log')\n",
        "    parser.add_argument('--model_dir', type=str, default='./models')\n",
        "    parser.add_argument('--world_model_path', type=str, default=\"CarRacingWorldModel\")\n",
        "\n",
        "    parser.add_argument('--total_timesteps', type=int, default=int(1e6))\n",
        "    parser.add_argument('--max_eval_iters', type=int, default=int(1e3))\n",
        "\n",
        "    parser.add_argument('--render', type=str2bool, default=True, help='Render frames')\n",
        "    parser.add_argument('--debug', type=str2bool, default=False, help='See debugging info')\n",
        "\n",
        "    args = parser.parse_args()\n",
        "    return args"
      ],
      "metadata": {
        "id": "EzLqnTLdeSQh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# utils.py\n",
        "# Code is from OpenAI Baseline and Tensor2Tensor\n",
        "\n",
        "import itertools\n",
        "import numpy as np\n",
        "from gym.envs.box2d import CarRacing\n",
        "import multiprocessing as mp\n",
        "\n",
        "def printstar(string, num_stars=50):\n",
        "    print(\"*\" * num_stars)\n",
        "    print(string)\n",
        "    print(\"*\" * num_stars)\n",
        "\n",
        "def make_env():\n",
        "    def _thunk():\n",
        "        env = CarRacing(grayscale=0, show_info_panel=0, discretize_actions=\"hard\", frames_per_state=1, num_lanes=1, num_tracks=1)\n",
        "        return env\n",
        "    return _thunk\n",
        "\n",
        "def worker(remote, parent_remote, env_fn_wrapper):\n",
        "    parent_remote.close()\n",
        "    env = env_fn_wrapper.x()\n",
        "    while True:\n",
        "        cmd, data = remote.recv()\n",
        "        if cmd == 'step':\n",
        "            ob, reward, done, info = env.step(data)\n",
        "            if done:\n",
        "                ob = env.reset()\n",
        "            remote.send((ob, reward, done, info))\n",
        "        elif cmd == 'reset':\n",
        "            ob = env.reset()\n",
        "            remote.send(ob)\n",
        "        elif cmd == 'reset_task':\n",
        "            ob = env.reset_task()\n",
        "            remote.send(ob)\n",
        "        elif cmd == 'close':\n",
        "            remote.close()\n",
        "            break\n",
        "        elif cmd == 'get_spaces':\n",
        "            remote.send((env.observation_space, env.action_space))\n",
        "        else:\n",
        "            raise NotImplementedError\n",
        "\n",
        "class VecEnv(object):\n",
        "    \"\"\"\n",
        "    An abstract asynchronous, vectorized environment.\n",
        "    \"\"\"\n",
        "    def __init__(self, num_envs, observation_space, action_space):\n",
        "        self.num_envs = num_envs\n",
        "        self.observation_space = observation_space\n",
        "        self.action_space = action_space\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"\n",
        "        Reset all the environments and return an array of\n",
        "        observations, or a tuple of observation arrays.\n",
        "        If step_async is still doing work, that work will\n",
        "        be cancelled and step_wait() should not be called\n",
        "        until step_async() is invoked again.\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    def step_async(self, actions):\n",
        "        \"\"\"\n",
        "        Tell all the environments to start taking a step\n",
        "        with the given actions.\n",
        "        Call step_wait() to get the results of the step.\n",
        "        You should not call this if a step_async run is\n",
        "        already pending.\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    def step_wait(self):\n",
        "        \"\"\"\n",
        "        Wait for the step taken with step_async().\n",
        "        Returns (obs, rews, dones, infos):\n",
        "         - obs: an array of observations, or a tuple of\n",
        "                arrays of observations.\n",
        "         - rews: an array of rewards\n",
        "         - dones: an array of \"episode done\" booleans\n",
        "         - infos: a sequence of info objects\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    def close(self):\n",
        "        \"\"\"\n",
        "        Clean up the environments' resources.\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    def step(self, actions):\n",
        "        self.step_async(actions)\n",
        "        return self.step_wait()\n",
        "\n",
        "\n",
        "class CloudpickleWrapper(object):\n",
        "    \"\"\"\n",
        "    Uses cloudpickle to serialize contents (otherwise multiprocessing tries to use pickle)\n",
        "    \"\"\"\n",
        "    def __init__(self, x):\n",
        "        self.x = x\n",
        "    def __getstate__(self):\n",
        "        import cloudpickle\n",
        "        return cloudpickle.dumps(self.x)\n",
        "    def __setstate__(self, ob):\n",
        "        import pickle\n",
        "        self.x = pickle.loads(ob)\n",
        "\n",
        "class SubprocVecEnv(VecEnv):\n",
        "    def __init__(self, env_fns, spaces=None):\n",
        "        \"\"\"\n",
        "        envs: list of gym environments to run in subprocesses\n",
        "        \"\"\"\n",
        "        self.waiting = False\n",
        "        self.closed = False\n",
        "        nenvs = len(env_fns)\n",
        "        self.nenvs = nenvs\n",
        "        self.remotes, self.work_remotes = zip(*[mp.Pipe() for _ in range(nenvs)])\n",
        "        self.ps = [mp.Process(target=worker, args=(work_remote, remote, CloudpickleWrapper(env_fn)))\n",
        "            for (work_remote, remote, env_fn) in zip(self.work_remotes, self.remotes, env_fns)]\n",
        "        for p in self.ps:\n",
        "            p.daemon = True # if the main process crashes, we should not cause things to hang\n",
        "            p.start()\n",
        "        for remote in self.work_remotes:\n",
        "            remote.close()\n",
        "\n",
        "        self.remotes[0].send(('get_spaces', None))\n",
        "        observation_space, action_space = self.remotes[0].recv()\n",
        "        VecEnv.__init__(self, len(env_fns), observation_space, action_space)\n",
        "\n",
        "    def step_async(self, actions):\n",
        "        if(type(actions) == int):\n",
        "            for remote in self.remotes:\n",
        "                remote.send(('step', actions))\n",
        "        else:\n",
        "            for remote, action in zip(self.remotes, actions):\n",
        "                remote.send(('step', action))\n",
        "        self.waiting = True\n",
        "\n",
        "    def step_wait(self):\n",
        "        results = [remote.recv() for remote in self.remotes]\n",
        "        self.waiting = False\n",
        "        obs, rews, dones, infos = zip(*results)\n",
        "        return np.stack(obs), np.stack(rews), np.stack(dones), infos\n",
        "\n",
        "    def reset(self):\n",
        "        for remote in self.remotes:\n",
        "            remote.send(('reset', None))\n",
        "        return np.stack([remote.recv() for remote in self.remotes])\n",
        "\n",
        "    def reset_task(self):\n",
        "        for remote in self.remotes:\n",
        "            remote.send(('reset_task', None))\n",
        "        return np.stack([remote.recv() for remote in self.remotes])\n",
        "\n",
        "    def close(self):\n",
        "        if self.closed:\n",
        "            return\n",
        "        if self.waiting:\n",
        "            for remote in self.remotes:\n",
        "                remote.recv()\n",
        "        for remote in self.remotes:\n",
        "            remote.send(('close', None))\n",
        "        for p in self.ps:\n",
        "            p.join()\n",
        "            self.closed = True\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.nenvs\n",
        "\n",
        "\n",
        "\n",
        "def shape_list(x):\n",
        "  \"\"\"Return list of dims, statically where possible.\"\"\"\n",
        "  x = tf.convert_to_tensor(x)\n",
        "\n",
        "  # If unknown rank, return dynamic shape\n",
        "  if x.get_shape().dims is None:\n",
        "    return tf.shape(x)\n",
        "\n",
        "  static = x.get_shape().as_list()\n",
        "  shape = tf.shape(x)\n",
        "\n",
        "  ret = []\n",
        "  for i, dim in enumerate(static):\n",
        "    if dim is None:\n",
        "      dim = shape[i]\n",
        "    ret.append(dim)\n",
        "  return ret\n",
        "\n",
        "def to_float(x):\n",
        "  \"\"\"Cast x to float; created because tf.to_float is deprecated.\"\"\"\n",
        "  return tf.cast(x, tf.float32)\n",
        "\n",
        "def cast_like(x, y):\n",
        "  \"\"\"Cast x to y's dtype, if necessary.\"\"\"\n",
        "  x = tf.convert_to_tensor(x)\n",
        "  y = tf.convert_to_tensor(y)\n",
        "\n",
        "  if x.dtype.base_dtype == y.dtype.base_dtype:\n",
        "    return x\n",
        "\n",
        "  cast_x = tf.cast(x, y.dtype)\n",
        "  if cast_x.device != x.device:\n",
        "    x_name = \"(eager Tensor)\"\n",
        "    try:\n",
        "      x_name = x.name\n",
        "    except AttributeError:\n",
        "      pass\n",
        "    tf.logging.warning(\"Cast for %s may induce copy from '%s' to '%s'\", x_name,\n",
        "                       x.device, cast_x.device)\n",
        "  return cast_x\n",
        "\n",
        "def layer_norm_vars(filters):\n",
        "  \"\"\"Create Variables for layer norm.\"\"\"\n",
        "  scale = tf.get_variable(\n",
        "      \"layer_norm_scale\", [filters], initializer=tf.ones_initializer())\n",
        "  bias = tf.get_variable(\n",
        "      \"layer_norm_bias\", [filters], initializer=tf.zeros_initializer())\n",
        "  return scale, bias\n",
        "\n",
        "\n",
        "def layer_norm_compute(x, epsilon, scale, bias, layer_collection=None):\n",
        "  \"\"\"Layer norm raw computation.\"\"\"\n",
        "\n",
        "  # Save these before they get converted to tensors by the casting below\n",
        "  params = (scale, bias)\n",
        "\n",
        "  epsilon, scale, bias = [cast_like(t, x) for t in [epsilon, scale, bias]]\n",
        "  mean = tf.reduce_mean(x, axis=[-1], keepdims=True)\n",
        "  variance = tf.reduce_mean(\n",
        "      tf.squared_difference(x, mean), axis=[-1], keepdims=True)\n",
        "  norm_x = (x - mean) * tf.rsqrt(variance + epsilon)\n",
        "\n",
        "  output = norm_x * scale + bias\n",
        "\n",
        "\n",
        "  return output\n",
        "\n",
        "def layer_norm(x,\n",
        "               filters=None,\n",
        "               epsilon=1e-6,\n",
        "               name=None,\n",
        "               reuse=None,\n",
        "               layer_collection=None):\n",
        "  \"\"\"Layer normalize the tensor x, averaging over the last dimension.\"\"\"\n",
        "  if filters is None:\n",
        "    filters = shape_list(x)[-1]\n",
        "  with tf.variable_scope(\n",
        "      name, default_name=\"layer_norm\", values=[x], reuse=reuse):\n",
        "    scale, bias = layer_norm_vars(filters)\n",
        "    return layer_norm_compute(x, epsilon, scale, bias,\n",
        "                              layer_collection=layer_collection)\n",
        "\n",
        "def standardize_images(x):\n",
        "  \"\"\"Image standardization on batches and videos.\"\"\"\n",
        "  with tf.name_scope(\"standardize_images\", values=[x]):\n",
        "    x_shape = shape_list(x)\n",
        "    x = to_float(tf.reshape(x, [-1] + x_shape[-3:]))\n",
        "    x_mean = tf.reduce_mean(x, axis=[1, 2], keepdims=True)\n",
        "    x_variance = tf.reduce_mean(\n",
        "        tf.squared_difference(x, x_mean), axis=[1, 2], keepdims=True)\n",
        "    num_pixels = to_float(x_shape[-2] * x_shape[-3])\n",
        "    x = (x - x_mean) / tf.maximum(tf.sqrt(x_variance), tf.rsqrt(num_pixels))\n",
        "    return tf.reshape(x, x_shape)\n",
        "\n",
        "def pad_to_same_length(x, y, final_length_divisible_by=1, axis=1):\n",
        "  \"\"\"Pad tensors x and y on axis 1 so that they have the same length.\"\"\"\n",
        "  if axis not in [1, 2]:\n",
        "    raise ValueError(\"Only axis=1 and axis=2 supported for now.\")\n",
        "  with tf.name_scope(\"pad_to_same_length\", values=[x, y]):\n",
        "    x_length = shape_list(x)[axis]\n",
        "    y_length = shape_list(y)[axis]\n",
        "    if (isinstance(x_length, int) and isinstance(y_length, int) and\n",
        "        x_length == y_length and final_length_divisible_by == 1):\n",
        "      return x, y\n",
        "    max_length = tf.maximum(x_length, y_length)\n",
        "    if final_length_divisible_by > 1:\n",
        "      # Find the nearest larger-or-equal integer divisible by given number.\n",
        "      max_length += final_length_divisible_by - 1\n",
        "      max_length //= final_length_divisible_by\n",
        "      max_length *= final_length_divisible_by\n",
        "    length_diff1 = max_length - x_length\n",
        "    length_diff2 = max_length - y_length\n",
        "\n",
        "    def padding_list(length_diff, arg):\n",
        "      if axis == 1:\n",
        "        return [[[0, 0], [0, length_diff]],\n",
        "                tf.zeros([tf.rank(arg) - 2, 2], dtype=tf.int32)]\n",
        "      return [[[0, 0], [0, 0], [0, length_diff]],\n",
        "              tf.zeros([tf.rank(arg) - 3, 2], dtype=tf.int32)]\n",
        "\n",
        "    paddings1 = tf.concat(padding_list(length_diff1, x), axis=0)\n",
        "    paddings2 = tf.concat(padding_list(length_diff2, y), axis=0)\n",
        "    res_x = tf.pad(x, paddings1)\n",
        "    res_y = tf.pad(y, paddings2)\n",
        "    # Static shapes are the same except for axis=1.\n",
        "    x_shape = x.shape.as_list()\n",
        "    x_shape[axis] = None\n",
        "    res_x.set_shape(x_shape)\n",
        "    y_shape = y.shape.as_list()\n",
        "    y_shape[axis] = None\n",
        "    res_y.set_shape(y_shape)\n",
        "    return res_x, res_y\n",
        "\n",
        "def make_even_size(x):\n",
        "  \"\"\"Pad x to be even-sized on axis 1 and 2, but only if necessary.\"\"\"\n",
        "  x_shape = x.get_shape().as_list()\n",
        "  assert len(x_shape) > 2, \"Only 3+-dimensional tensors supported.\"\n",
        "  shape = [dim if dim is not None else -1 for dim in x_shape]\n",
        "  new_shape = x_shape  # To make sure constant shapes remain constant.\n",
        "  if x_shape[1] is not None:\n",
        "    new_shape[1] = 2 * int(math.ceil(x_shape[1] * 0.5))\n",
        "  if x_shape[2] is not None:\n",
        "    new_shape[2] = 2 * int(math.ceil(x_shape[2] * 0.5))\n",
        "  if shape[1] % 2 == 0 and shape[2] % 2 == 0:\n",
        "    return x\n",
        "  if shape[1] % 2 == 0:\n",
        "    x, _ = pad_to_same_length(x, x, final_length_divisible_by=2, axis=2)\n",
        "    x.set_shape(new_shape)\n",
        "    return x\n",
        "  if shape[2] % 2 == 0:\n",
        "    x, _ = pad_to_same_length(x, x, final_length_divisible_by=2, axis=1)\n",
        "    x.set_shape(new_shape)\n",
        "    return x\n",
        "  x, _ = pad_to_same_length(x, x, final_length_divisible_by=2, axis=1)\n",
        "  x, _ = pad_to_same_length(x, x, final_length_divisible_by=2, axis=2)\n",
        "  x.set_shape(new_shape)\n",
        "  return x\n",
        "\n",
        "\n",
        "def add_timing_signal_nd(x, min_timescale=1.0, max_timescale=1.0e4):\n",
        "  \"\"\"Adds a bunch of sinusoids of different frequencies to a Tensor.\n",
        "\n",
        "  Each channel of the input Tensor is incremented by a sinusoid of a different\n",
        "  frequency and phase in one of the positional dimensions.\n",
        "\n",
        "  This allows attention to learn to use absolute and relative positions.\n",
        "  Timing signals should be added to some precursors of both the query and the\n",
        "  memory inputs to attention.\n",
        "\n",
        "  The use of relative position is possible because sin(a+b) and cos(a+b) can be\n",
        "  expressed in terms of b, sin(a) and cos(a).\n",
        "\n",
        "  x is a Tensor with n \"positional\" dimensions, e.g. one dimension for a\n",
        "  sequence or two dimensions for an image\n",
        "\n",
        "  We use a geometric sequence of timescales starting with\n",
        "  min_timescale and ending with max_timescale.  The number of different\n",
        "  timescales is equal to channels // (n * 2). For each timescale, we\n",
        "  generate the two sinusoidal signals sin(timestep/timescale) and\n",
        "  cos(timestep/timescale).  All of these sinusoids are concatenated in\n",
        "  the channels dimension.\n",
        "\n",
        "  Args:\n",
        "    x: a Tensor with shape [batch, d1 ... dn, channels]\n",
        "    min_timescale: a float\n",
        "    max_timescale: a float\n",
        "\n",
        "  Returns:\n",
        "    a Tensor the same shape as x.\n",
        "  \"\"\"\n",
        "  num_dims = len(x.get_shape().as_list()) - 2\n",
        "  channels = shape_list(x)[-1]\n",
        "  num_timescales = channels // (num_dims * 2)\n",
        "  log_timescale_increment = (\n",
        "      math.log(float(max_timescale) / float(min_timescale)) /\n",
        "      (tf.to_float(num_timescales) - 1))\n",
        "  inv_timescales = min_timescale * tf.exp(\n",
        "      tf.to_float(tf.range(num_timescales)) * -log_timescale_increment)\n",
        "  for dim in range(num_dims):\n",
        "    length = shape_list(x)[dim + 1]\n",
        "    position = tf.to_float(tf.range(length))\n",
        "    scaled_time = tf.expand_dims(position, 1) * tf.expand_dims(\n",
        "        inv_timescales, 0)\n",
        "    signal = tf.concat([tf.sin(scaled_time), tf.cos(scaled_time)], axis=1)\n",
        "    prepad = dim * 2 * num_timescales\n",
        "    postpad = channels - (dim + 1) * 2 * num_timescales\n",
        "    signal = tf.pad(signal, [[0, 0], [prepad, postpad]])\n",
        "    for _ in range(1 + dim):\n",
        "      signal = tf.expand_dims(signal, 0)\n",
        "    for _ in range(num_dims - 1 - dim):\n",
        "      signal = tf.expand_dims(signal, -2)\n",
        "    x += signal\n",
        "  return x\n"
      ],
      "metadata": {
        "id": "554YKfW8eVHK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}